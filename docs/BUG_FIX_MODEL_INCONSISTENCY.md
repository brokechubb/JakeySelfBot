# Bug Fix - Two Different Models Used (Jan 4, 2026)

## Issue Report

**Symptom:** Bot uses one model for initial call, different model for follow-up

**Evidence from logs:**
```
16:51:58 INFO ai.openrouter model=openai/gpt-oss-120b:free (CORRECT - supports tools)
16:52:47 INFO Follow-up response: 'model': 'meta-llama/llama-3.3-70b-instruct:free' (WRONG - no tools)
```

## Root Cause

**Two conflicting model configuration variables:**

### `config.py`
```python
# Line 10 - Used by bot client
DEFAULT_MODEL = "meta-llama/llama-3.3-70b-instruct:free"  ‚ùå Old, no function calling

# Line 16 - Used by OpenRouter API
OPENROUTER_DEFAULT_MODEL = "openai/gpt-oss-120b:free"  ‚úÖ New, supports function calling
```

### `bot/client.py:547`
```python
self.current_model = DEFAULT_MODEL  # Uses the WRONG variable!
```

## The Problem

1. **Initial AI call** ‚Üí OpenRouter auto-fallback detects tools ‚Üí Uses `openai/gpt-oss-120b:free` ‚úÖ
2. **Follow-up call** ‚Üí Uses `self.current_model` ‚Üí Uses `meta-llama/llama-3.3-70b-instruct:free` ‚ùå
3. **Result:** Inconsistent models, follow-up fails with function calling

## Fix Applied

Changed `DEFAULT_MODEL` default value in `config.py`:

**Before:**
```python
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "meta-llama/llama-3.3-70b-instruct:free")
```

**After:**
```python
# DEPRECATED: Use OPENROUTER_DEFAULT_MODEL instead
# DEFAULT_MODEL kept for backward compatibility but should not be used
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-oss-120b:free")
```

## Impact

**Before:**
- Initial call: `openai/gpt-oss-120b:free` (via auto-fallback)
- Follow-up call: `meta-llama/llama-3.3-70b-instruct:free` (via DEFAULT_MODEL)
- **Result:** Inconsistent behavior, follow-up might fail

**After:**
- Initial call: `openai/gpt-oss-120b:free`
- Follow-up call: `openai/gpt-oss-120b:free`
- **Result:** Consistent, reliable function calling

## Why This Happened

Historical config evolution:
1. Originally used `DEFAULT_MODEL` for Pollinations
2. Added `OPENROUTER_DEFAULT_MODEL` when OpenRouter became primary
3. Bot client still referenced old `DEFAULT_MODEL`
4. Auto-fallback logic masked the issue for initial calls
5. Follow-up calls exposed the inconsistency

## Future Improvement

**Option 1:** Remove `DEFAULT_MODEL` entirely, use only `OPENROUTER_DEFAULT_MODEL`

**Option 2:** Update bot client to use `OPENROUTER_DEFAULT_MODEL`:
```python
# bot/client.py:547
from config import OPENROUTER_DEFAULT_MODEL
self.current_model = OPENROUTER_DEFAULT_MODEL
```

**Recommendation:** Option 2 is safer for backward compatibility

## Files Modified

1. **`config.py:10`** - Changed DEFAULT_MODEL default from llama-3.3 to gpt-oss-120b

## Testing

Restart bot and check logs - both calls should now use same model:
```
INFO model=openai/gpt-oss-120b:free (initial)
INFO 'model': 'openai/gpt-oss-120b:free' (follow-up)
```

## All Fixes Today (Complete List)

1. ‚úÖ Duplicate API call - Fixed retry loop
2. ‚úÖ Model incompatibility - Auto-fallback logic
3. ‚úÖ Warning spam - Changed to DEBUG
4. ‚úÖ Multi-round tool calls - Detection + feedback
5. ‚úÖ Silent failures - Error messages
6. ‚úÖ Current channel - Tool descriptions updated
7. ‚úÖ **Model inconsistency** - DEFAULT_MODEL synchronized

All fixes ensure consistent, reliable function calling! üéØ
